{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image1', 'image2', 'class'],\n",
       "        num_rows: 4508\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image1', 'image2', 'class'],\n",
       "        num_rows: 92\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./image_gt.json\", split=\"train\")#[:5%]\n",
    "datasets = dataset.train_test_split(test_size=0.02)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image1', 'image2', 'class'],\n",
       "    num_rows: 4508\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = datasets[\"train\"]\n",
    "test_ds = datasets[\"test\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pair-25-B-Single-EYE_trial21_player.jpg', 'Pair-25-Coop-EYE_trial39_playerA.jpg', 'Pair-25-A-Single-EYE_trial16_player.jpg', 'Pair-26-Coop-EYE_trial28_playerA.jpg', 'Pair-27-Comp-EYE_trial07_playerA.jpg']\n",
      "['Pair-25-B-Single-EYE_trial21_observer.jpg', 'Pair-25-Coop-EYE_trial39_playerB.jpg', 'Pair-25-A-Single-EYE_trial16_observer.jpg', 'Pair-26-Coop-EYE_trial28_playerB.jpg', 'Pair-27-Comp-EYE_trial07_playerB.jpg']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './heatmapOn_trajOn/Pair-25-B-Single-EYE_trial21_player.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_images2_to_visualize)\n\u001b[1;32m     37\u001b[0m sample_captions \u001b[38;5;241m=\u001b[39m [train_ds[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)]\n\u001b[0;32m---> 38\u001b[0m plot_images(sample_images1_to_visualize, sample_images2_to_visualize, sample_captions)\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mplot_images\u001b[0;34m(images1, images2, captions)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images1)):\n\u001b[1;32m     10\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./heatmapOn_trajOn/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m images1[i]\n\u001b[0;32m---> 11\u001b[0m     image1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[1;32m     12\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./heatmapOn_trajOn/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m images2[i]\n\u001b[1;32m     13\u001b[0m     image2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging/lib/python3.12/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './heatmapOn_trajOn/Pair-25-B-Single-EYE_trial21_player.jpg'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plot_images(images1, images2, captions):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images1)):\n",
    "        img_path = \"./heatmapOn_trajOn/\" + images1[i]\n",
    "        image1 = Image.open(img_path)\n",
    "        img_path = \"./heatmapOn_trajOn/\" + images2[i]\n",
    "        image2 = Image.open(img_path)\n",
    "        \n",
    "        image2 = image2.resize((image1.width, image1.height))\n",
    "        # 創建拼接後的新圖片（寬度為兩張圖片寬度之和）\n",
    "        new_width = image1.width\n",
    "        new_height = image1.height + image2.height\n",
    "        concatenated_image = Image.new('RGB', (new_width, new_height))\n",
    "        \n",
    "        # 將圖片粘貼到新圖片上\n",
    "        concatenated_image.paste(image1, (0, 0))\n",
    "        concatenated_image.paste(image2, (0, image1.height))\n",
    "\n",
    "        ax = plt.subplot(1, len(images1), i + 1)\n",
    "        caption = captions[i]\n",
    "        caption = \"\\n\".join(wrap(caption, 12))\n",
    "        plt.title(caption)\n",
    "        plt.imshow(concatenated_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "sample_images1_to_visualize = [train_ds[i][\"image1\"] for i in range(5)]\n",
    "sample_images2_to_visualize = [train_ds[i][\"image2\"] for i in range(5)]\n",
    "print(sample_images1_to_visualize)\n",
    "print(sample_images2_to_visualize)\n",
    "sample_captions = [train_ds[i][\"class\"] for i in range(5)]\n",
    "plot_images(sample_images1_to_visualize, sample_images2_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 101, 2309,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'pixel_values': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]]), 'labels': tensor([ 101, 2309,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "#processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "#processor = GitProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "\n",
    "def transforms(example_batch):\n",
    "    root = \"./heatmapOn_trajOn/\"\n",
    "    \n",
    "    concatenated_images = []\n",
    "    #print(example_batch[\"image1\"])\n",
    "    \n",
    "    # 拼接兩張圖片\n",
    "    for img_path1, img_path2 in zip(example_batch[\"image1\"], example_batch[\"image2\"]):\n",
    "        try:\n",
    "            image1 = Image.open(root + img_path1)\n",
    "            image2 = Image.open(root + img_path2)\n",
    "            # 確保兩張圖片的尺寸相同（可選）\n",
    "            image2 = image2.resize((image1.width, image1.height))\n",
    "            black_image = Image.new('RGB', (image1.width, image1.height), color=(0, 0, 0))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading images {img_path1} and {img_path2}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        #concatenated_images.append([image1, black_image, black_image, image2, black_image, black_image])\n",
    "        concatenated_images.append([image1, image2])\n",
    "        \n",
    "    \n",
    "    # 加載文本標籤\n",
    "    captions = [x for x in example_batch[\"class\"]]\n",
    "    \n",
    "    # 將拼接後的圖片和標籤進行處理\n",
    "    inputs = processor(images=concatenated_images, text=captions, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "    #print(f\"Pixel values shape: {inputs['pixel_values'].shape}\")\n",
    "    #print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "# 設定 transforms 給 train 和 test 資料集\n",
    "train_ds.set_transform(transforms)\n",
    "test_ds.set_transform(transforms)\n",
    "\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/git-base-vatex were not used when initializing GitForCausalLM: ['git.img_temperal_embedding.2', 'git.img_temperal_embedding.3', 'git.img_temperal_embedding.4', 'git.img_temperal_embedding.5']\n",
      "- This IS expected if you are initializing GitForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GitForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitConfig {\n",
      "  \"_name_or_path\": \"microsoft/git-base-vatex\",\n",
      "  \"architectures\": [\n",
      "    \"GitForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"git\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_image_with_embedding\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vision_config\": {\n",
      "    \"dropout\": 0.0,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"model_type\": \"git_vision_model\",\n",
      "    \"projection_dim\": 512\n",
      "  },\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "GitForCausalLM(\n",
      "  (git): GitModel(\n",
      "    (embeddings): GitEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(1024, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (image_encoder): GitVisionModel(\n",
      "      (vision_model): GitVisionTransformer(\n",
      "        (embeddings): GitVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "          (position_embedding): Embedding(197, 768)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): GitVisionEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x GitVisionEncoderLayer(\n",
      "              (self_attn): GitVisionAttention(\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): GitVisionMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (encoder): GitEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x GitLayer(\n",
      "          (attention): GitAttention(\n",
      "            (self): GitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): GitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): GitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): GitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visual_projection): GitProjection(\n",
      "      (visual_projection): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (img_temperal_embedding): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 1x1x768]\n",
      "        (1): Parameter containing: [torch.float32 of size 1x1x768]\n",
      "    )\n",
      "  )\n",
      "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, GitConfig\n",
    "\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
    "configuration = GitConfig.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "configuration.num_image_with_embedding = 2\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\", config=configuration)\n",
    "#model.config.num_image_with_embedding = 2\n",
    "print(model.config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load the metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n",
    "f1_metric = load(\"f1\")\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        predicted = logits.argmax(-1)\n",
    "\n",
    "        decoded_labels = [label.lower() for label in processor.batch_decode(labels, skip_special_tokens=True)]\n",
    "        decoded_predictions = [pred.lower() for pred in processor.batch_decode(predicted, skip_special_tokens=True)]\n",
    "\n",
    "        label_mapping = {\"cooperation\": 0, \"single\": 1, \"competition\": 2}\n",
    "        encoded_labels = [label_mapping.get(label, 4) for label in decoded_labels]\n",
    "        encoded_predictions = [label_mapping.get(pred, 4) for pred in decoded_predictions]\n",
    "\n",
    "        # Calculate each metric\n",
    "        accuracy = accuracy_metric.compute(predictions=encoded_predictions, references=encoded_labels)\n",
    "        precision = precision_metric.compute(predictions=encoded_predictions, references=encoded_labels, average=\"weighted\")\n",
    "        recall = recall_metric.compute(predictions=encoded_predictions, references=encoded_labels, average=\"weighted\")\n",
    "        f1_score = f1_metric.compute(predictions=encoded_predictions, references=encoded_labels, average=\"weighted\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\n",
    "        \"accuracy\": accuracy['accuracy'],\n",
    "        \"precision\": precision['precision'],\n",
    "        \"recall\": recall['recall'],\n",
    "        \"f1_score\": f1_score['f1']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vqa_check\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=16,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnelabai/anaconda3/envs/hugging/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2b31af988f42bc821d1b3cecb93fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9479, 'grad_norm': 38.22489929199219, 'learning_rate': 4.821167883211679e-05, 'epoch': 0.36}\n",
      "{'loss': 1.9457, 'grad_norm': 10.135743141174316, 'learning_rate': 4.6386861313868616e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0919, 'grad_norm': 0.204263836145401, 'learning_rate': 4.456204379562044e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0378, 'grad_norm': 0.21828122437000275, 'learning_rate': 4.273722627737227e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a19d47974a349cbae95564ab7f211f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03540249168872833, 'eval_runtime': 5.8601, 'eval_samples_per_second': 15.358, 'eval_steps_per_second': 15.358, 'epoch': 1.46}\n",
      "{'loss': 0.0361, 'grad_norm': 0.4641824960708618, 'learning_rate': 4.091240875912409e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0368, 'grad_norm': 0.3566315174102783, 'learning_rate': 3.908759124087591e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0364, 'grad_norm': 0.2587302327156067, 'learning_rate': 3.726277372262774e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0352, 'grad_norm': 0.06819877028465271, 'learning_rate': 3.5437956204379565e-05, 'epoch': 2.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a66da69148b4497be6d9ba60f754d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03343524783849716, 'eval_runtime': 5.9212, 'eval_samples_per_second': 15.2, 'eval_steps_per_second': 15.2, 'epoch': 2.91}\n",
      "{'loss': 0.0346, 'grad_norm': 0.27490970492362976, 'learning_rate': 3.361313868613139e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0342, 'grad_norm': 0.39670664072036743, 'learning_rate': 3.178832116788321e-05, 'epoch': 3.64}\n",
      "{'loss': 0.0342, 'grad_norm': 0.09060075134038925, 'learning_rate': 2.996350364963504e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0347, 'grad_norm': 0.14605297148227692, 'learning_rate': 2.813868613138686e-05, 'epoch': 4.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c363a02b24548a1a50117598ed2db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03389500454068184, 'eval_runtime': 5.8161, 'eval_samples_per_second': 15.474, 'eval_steps_per_second': 15.474, 'epoch': 4.37}\n",
      "{'loss': 0.0344, 'grad_norm': 0.19812078773975372, 'learning_rate': 2.6313868613138688e-05, 'epoch': 4.74}\n",
      "{'loss': 0.0346, 'grad_norm': 0.1126413494348526, 'learning_rate': 2.448905109489051e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0346, 'grad_norm': 0.29561519622802734, 'learning_rate': 2.2664233576642337e-05, 'epoch': 5.46}\n",
      "{'loss': 0.0342, 'grad_norm': 0.23871533572673798, 'learning_rate': 2.0839416058394163e-05, 'epoch': 5.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2292b25807b459cb13f41d1031678fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03387824073433876, 'eval_runtime': 6.0042, 'eval_samples_per_second': 14.99, 'eval_steps_per_second': 14.99, 'epoch': 5.83}\n",
      "{'loss': 0.0349, 'grad_norm': 0.10572350025177002, 'learning_rate': 1.9014598540145986e-05, 'epoch': 6.19}\n",
      "{'loss': 0.0337, 'grad_norm': 0.17494644224643707, 'learning_rate': 1.718978102189781e-05, 'epoch': 6.56}\n",
      "{'loss': 0.0346, 'grad_norm': 0.28352829813957214, 'learning_rate': 1.5364963503649634e-05, 'epoch': 6.92}\n",
      "{'loss': 0.0345, 'grad_norm': 0.19742584228515625, 'learning_rate': 1.3540145985401462e-05, 'epoch': 7.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36181847a024bfc874f950215ed6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.033545803278684616, 'eval_runtime': 5.8092, 'eval_samples_per_second': 15.493, 'eval_steps_per_second': 15.493, 'epoch': 7.29}\n",
      "{'loss': 0.0338, 'grad_norm': 0.13380731642246246, 'learning_rate': 1.1715328467153286e-05, 'epoch': 7.65}\n",
      "{'loss': 0.0339, 'grad_norm': 0.2839307188987732, 'learning_rate': 9.89051094890511e-06, 'epoch': 8.01}\n",
      "{'loss': 0.0322, 'grad_norm': 0.3772124648094177, 'learning_rate': 8.065693430656935e-06, 'epoch': 8.38}\n",
      "{'loss': 0.0321, 'grad_norm': 0.27192819118499756, 'learning_rate': 6.240875912408759e-06, 'epoch': 8.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80153ec39703469da0ae45901ad48e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03194621577858925, 'eval_runtime': 6.0193, 'eval_samples_per_second': 14.952, 'eval_steps_per_second': 14.952, 'epoch': 8.74}\n",
      "{'loss': 0.0321, 'grad_norm': 0.23210279643535614, 'learning_rate': 4.416058394160584e-06, 'epoch': 9.11}\n",
      "{'loss': 0.0316, 'grad_norm': 0.3105209767818451, 'learning_rate': 2.591240875912409e-06, 'epoch': 9.47}\n",
      "{'loss': 0.0317, 'grad_norm': 0.4236796498298645, 'learning_rate': 7.664233576642336e-07, 'epoch': 9.84}\n",
      "{'train_runtime': 3546.8358, 'train_samples_per_second': 12.377, 'train_steps_per_second': 0.386, 'train_loss': 0.35842943587442383, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1370, training_loss=0.35842943587442383, metrics={'train_runtime': 3546.8358, 'train_samples_per_second': 12.377, 'train_steps_per_second': 0.386, 'total_flos': 1280937208303872.0, 'train_loss': 0.35842943587442383, 'epoch': 9.981785063752277})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
