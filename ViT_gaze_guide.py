import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import ViTForImageClassification, ViTImageProcessor
from datasets import load_dataset
from PIL import Image
import numpy as np
from tqdm import tqdm
import os
import json


# ==================== Gaze-Guided Attention æ©Ÿåˆ¶ ====================

class GazeWeightComputer:
    """è¨ˆç®— Gaze æ¬Šé‡ã€é®ç½©å’Œ bias"""
    
    @staticmethod
    def compute_gaze_weights(heatmap, eeg_scalar=0.5, top_p=0.7, alpha=1.0, 
                            patch_size=16, img_size=224):
        """
        å¾ gaze heatmap è¨ˆç®— token æ¬Šé‡å’Œé®ç½©
        
        Args:
            heatmap: (B, 1, H, W) gaze heatmap
            eeg_scalar: EEG æ¨™é‡ [0,1] æˆ– (B,)
            top_p: ä¿ç•™å‰ p% çš„ tokens
            alpha: attention bias çš„ç¸®æ”¾ä¿‚æ•¸
            patch_size: patch å¤§å°
            img_size: åœ–ç‰‡å¤§å°
            
        Returns:
            soft_weights: (B, num_patches+1) æ­¸ä¸€åŒ–æ¬Šé‡ï¼ˆå«CLSï¼‰
            binary_mask: (B, num_patches+1) äºŒå€¼é®ç½©ï¼ˆå«CLSï¼‰
            attn_bias: (B, num_patches+1) attention bias
        """
        B = heatmap.shape[0]
        num_patches_side = img_size // patch_size
        num_patches = num_patches_side ** 2
        device = heatmap.device
        
        # (A-1) å¹³å‡æ± åŒ–åˆ° patch grid
        pooled_heatmap = F.adaptive_avg_pool2d(
            heatmap, 
            (num_patches_side, num_patches_side)
        )  # (B, 1, H//P, W//P)
        
        # å±•å¹³ç‚º token åºåˆ—
        patch_weights = pooled_heatmap.flatten(2).squeeze(1)  # (B, num_patches)
        
        # (A-2) è¨ˆç®—æº«åº¦ï¼šeeg_scalar é«˜ â†’ tau å° â†’ æ›´å°–éŠ³
        if isinstance(eeg_scalar, (int, float)):
            tau = 1.5 - eeg_scalar * 1.2
            tau = torch.tensor(tau, device=device)
        else:
            eeg_scalar = eeg_scalar.to(device)
            tau = 1.5 - eeg_scalar * 1.2
        
        tau = torch.clamp(tau, 0.3, 1.5)
        
        # æº«åº¦ç¸®æ”¾çš„ softmax
        if tau.dim() == 0:
            tau = tau.unsqueeze(0)
        tau = tau.view(B, 1)
        
        patch_weights_scaled = patch_weights / tau
        soft_weights_patches = F.softmax(patch_weights_scaled, dim=-1)  # (B, num_patches)
        
        # ç‚º CLS token æ·»åŠ æ¬Šé‡ï¼ˆå§‹çµ‚ä¿ç•™ï¼‰
        cls_weight = torch.ones(B, 1, device=device)
        soft_weights = torch.cat([cls_weight, soft_weights_patches], dim=1)  # (B, num_patches+1)
        
        # (A-3) Binary Mask: Top-p é¸æ“‡
        binary_mask = torch.zeros_like(soft_weights)
        binary_mask[:, 0] = 1.0  # CLS token å§‹çµ‚ä¿ç•™
        
        for b in range(B):
            sorted_weights, sorted_indices = torch.sort(
                soft_weights_patches[b], 
                descending=True
            )
            cumsum = torch.cumsum(sorted_weights, dim=0)
            
            # æ‰¾åˆ°ç´¯ç©å’Œè¶…é top_p çš„ä½ç½®
            cutoff_idx = torch.searchsorted(cumsum, top_p).item()
            cutoff_idx = max(1, min(cutoff_idx + 1, num_patches))
            
            # é¸æ“‡çš„ token ç´¢å¼•ï¼ˆ+1 å› ç‚º CLS åœ¨ç¬¬ 0 ä½ï¼‰
            selected_indices = sorted_indices[:cutoff_idx] + 1
            binary_mask[b, selected_indices] = 1.0
        
        # (A-4) Attention Bias
        eps = 1e-8
        attn_bias = alpha * torch.log(soft_weights + eps)  # (B, num_patches+1)
        
        return soft_weights, binary_mask, attn_bias


class GazeGuidedAttention(nn.Module):
    """Gaze-guided self-attention layer"""
    
    def __init__(self, hidden_size, num_heads, dropout=0.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        assert self.head_dim * num_heads == hidden_size
        
        # Q, K, V æŠ•å½±
        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=True)
        self.proj = nn.Linear(hidden_size, hidden_size)
        self.attn_dropout = nn.Dropout(dropout)
        self.proj_dropout = nn.Dropout(dropout)
    
    def forward(self, x, attn_bias=None):
        """
        Args:
            x: (B, N, D) token embeddings
            attn_bias: (B, N) attention bias for each token
        """
        B, N, D = x.shape
        
        # QKV projection and reshape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # (B) Scaled dot-product attention
        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, N, N)
        
        # Apply attention bias: broadcast bias to all heads and queries
        if attn_bias is not None:
            # attn_bias: (B, N) -> (B, 1, 1, N) for broadcasting to (B, num_heads, N, N)
            bias_expanded = attn_bias.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, N)
            attn_scores = attn_scores + bias_expanded
        
        # Softmax and dropout
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)
        
        # Apply attention to values
        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, N, D)
        
        # Output projection
        output = self.proj(attn_output)
        output = self.proj_dropout(output)
        
        return output


class GazeGuidedTransformerBlock(nn.Module):
    """Transformer block with gaze-guided attention"""
    
    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, dropout=0.0):
        super().__init__()
        self.attention = GazeGuidedAttention(hidden_size, num_heads, dropout)
        self.norm1 = nn.LayerNorm(hidden_size, eps=1e-6)
        self.norm2 = nn.LayerNorm(hidden_size, eps=1e-6)
        
        # MLP
        mlp_hidden = int(hidden_size * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, mlp_hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden, hidden_size),
            nn.Dropout(dropout)
        )
    
    def forward(self, x, attn_bias=None):
        # Attention with residual (Pre-LN)
        x = x + self.attention(self.norm1(x), attn_bias)
        # MLP with residual
        x = x + self.mlp(self.norm2(x))
        return x


# ==================== Token Pruning å·¥å…·å‡½æ•¸ ====================

def prune_tokens(x, mask):
    """
    (C) æ ¹æ“š mask å° tokens é€²è¡Œ pruning
    
    Args:
        x: (B, N, D) tokens
        mask: (B, N) binary mask
    
    Returns:
        pruned_x: List of tensors, æ¯å€‹ (N_kept, D)
        kept_indices: List of tensors, æ¯å€‹ä¿ç•™çš„ç´¢å¼•
    """
    B = x.shape[0]
    pruned_x = []
    kept_indices = []
    
    for b in range(B):
        indices = torch.nonzero(mask[b], as_tuple=False).squeeze(-1)
        pruned_x.append(x[b, indices, :])
        kept_indices.append(indices)
    
    return pruned_x, kept_indices


def restore_tokens(pruned_list, kept_indices_list, original_shape, device):
    """
    å°‡ pruned tokens é‚„åŸåˆ°åŸå§‹ä½ç½®
    
    Args:
        pruned_list: List of (N_kept, D) tensors
        kept_indices_list: List of kept indices
        original_shape: (B, N, D)
        device: torch device
    
    Returns:
        restored_x: (B, N, D)
    """
    B, N, D = original_shape
    restored_x = torch.zeros(B, N, D, device=device)
    
    for b in range(B):
        restored_x[b, kept_indices_list[b], :] = pruned_list[b]
    
    return restored_x


def pad_pruned_tokens(pruned_list):
    """
    å°‡ä¸åŒé•·åº¦çš„ pruned tokens padding åˆ°ç›¸åŒé•·åº¦
    
    Args:
        pruned_list: List of (N_kept, D) tensors
    
    Returns:
        padded_tokens: (B, max_N, D)
        padding_mask: (B, max_N) 1 è¡¨ç¤ºæœ‰æ•ˆï¼Œ0 è¡¨ç¤º padding
    """
    B = len(pruned_list)
    D = pruned_list[0].shape[-1]
    max_len = max(t.shape[0] for t in pruned_list)
    device = pruned_list[0].device
    
    padded_tokens = torch.zeros(B, max_len, D, device=device)
    padding_mask = torch.zeros(B, max_len, device=device)
    
    for b in range(B):
        length = pruned_list[b].shape[0]
        padded_tokens[b, :length] = pruned_list[b]
        padding_mask[b, :length] = 1.0
    
    return padded_tokens, padding_mask


# ==================== ä¸»æ¨¡å‹ ====================

class GazeGuidedViT(nn.Module):
    """ViT with Gaze-guided attention"""
    
    def __init__(self, base_model, patch_size=16, img_size=224, 
                 apply_gaze_from_layer=0, use_pruning=True):
        super().__init__()
        self.config = base_model.config
        self.patch_size = patch_size
        self.img_size = img_size
        self.apply_gaze_from_layer = apply_gaze_from_layer
        self.use_pruning_in_forward = use_pruning
        
        # ä¿ç•™ embeddings
        self.embeddings = base_model.vit.embeddings
        
        # å»ºç«‹ gaze-guided encoder blocks
        hidden_size = self.config.hidden_size
        num_heads = self.config.num_attention_heads
        num_layers = self.config.num_hidden_layers
        dropout = self.config.hidden_dropout_prob
        
        self.encoder_blocks = nn.ModuleList([
            GazeGuidedTransformerBlock(
                hidden_size, num_heads, 
                mlp_ratio=4.0, dropout=dropout
            ) for _ in range(num_layers)
        ])
        
        # è¤‡è£½åŸå§‹æ¬Šé‡
        self._copy_weights_from_base_model(base_model)
        
        self.layernorm = base_model.vit.layernorm
        self.classifier = base_model.classifier
        
        print(f"âœ“ Gaze-guided ViT åˆå§‹åŒ–å®Œæˆ")
        print(f"  - å±¤æ•¸: {num_layers}")
        print(f"  - å¾ç¬¬ {apply_gaze_from_layer} å±¤é–‹å§‹æ‡‰ç”¨ gaze bias")
        print(f"  - Token pruning: {'å•Ÿç”¨' if use_pruning else 'åœç”¨'}")
    
    def _copy_weights_from_base_model(self, base_model):
        """å¾åŸå§‹ ViT è¤‡è£½æ¬Šé‡"""
        with torch.no_grad():
            for i, block in enumerate(self.encoder_blocks):
                orig_block = base_model.vit.encoder.layer[i]
                
                # è¤‡è£½ attention QKV
                orig_attn = orig_block.attention.attention
                block.attention.qkv.weight.data = torch.cat([
                    orig_attn.query.weight.data,
                    orig_attn.key.weight.data,
                    orig_attn.value.weight.data
                ], dim=0)
                block.attention.qkv.bias.data = torch.cat([
                    orig_attn.query.bias.data,
                    orig_attn.key.bias.data,
                    orig_attn.value.bias.data
                ], dim=0)
                
                # è¤‡è£½ attention projection
                block.attention.proj.weight.data = orig_block.attention.output.dense.weight.data.clone()
                block.attention.proj.bias.data = orig_block.attention.output.dense.bias.data.clone()
                
                # è¤‡è£½ LayerNorm
                block.norm1.weight.data = orig_block.layernorm_before.weight.data.clone()
                block.norm1.bias.data = orig_block.layernorm_before.bias.data.clone()
                block.norm2.weight.data = orig_block.layernorm_after.weight.data.clone()
                block.norm2.bias.data = orig_block.layernorm_after.bias.data.clone()
                
                # è¤‡è£½ MLP
                block.mlp[0].weight.data = orig_block.intermediate.dense.weight.data.clone()
                block.mlp[0].bias.data = orig_block.intermediate.dense.bias.data.clone()
                block.mlp[3].weight.data = orig_block.output.dense.weight.data.clone()
                block.mlp[3].bias.data = orig_block.output.dense.bias.data.clone()
    
    def forward(self, pixel_values, heatmap, eeg_scalar=0.5, top_p=0.7, 
                alpha=1.0, labels=None):
        """
        Forward pass
        
        Args:
            pixel_values: (B, C, H, W)
            heatmap: (B, 1, H, W)
            eeg_scalar: float or (B,)
            top_p: float
            alpha: float
            labels: (B,) å¯é¸
        """
        device = pixel_values.device
        
        # è¨ˆç®— gaze æ¬Šé‡ã€mask å’Œ bias
        soft_weights, binary_mask, attn_bias = GazeWeightComputer.compute_gaze_weights(
            heatmap, eeg_scalar, top_p, alpha, 
            self.patch_size, self.img_size
        )
        
        # Embeddings
        x = self.embeddings(pixel_values)  # (B, 1+num_patches, D)
        B, N, D = x.shape
        
        # é€šéæ‰€æœ‰ encoder blocks
        for i, block in enumerate(self.encoder_blocks):
            # æ±ºå®šæ˜¯å¦æ‡‰ç”¨ gaze bias
            current_bias = attn_bias if i >= self.apply_gaze_from_layer else None
            
            # ç°¡åŒ–ç‰ˆï¼šä¸åœ¨ä¸­é–“åš pruningï¼Œåªç”¨ bias å¼•å°
            # å®Œæ•´ pruning éœ€è¦æ›´è¤‡é›œçš„å¯¦ä½œ
            x = block(x, current_bias)
        
        # LayerNorm
        x = self.layernorm(x)
        
        # Pooling
        if self.use_pruning_in_forward:
            # ä½¿ç”¨ mask åŠ æ¬Šå¹³å‡
            masked_x = x * binary_mask.unsqueeze(-1)
            pooled_output = masked_x.sum(dim=1) / binary_mask.sum(dim=1, keepdim=True).clamp(min=1)
        else:
            # ä½¿ç”¨ CLS token
            pooled_output = x[:, 0]
        
        # åˆ†é¡
        logits = self.classifier(pooled_output)
        
        # æå¤±
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)
        
        # çµ±è¨ˆä¿ç•™çš„ token æ•¸é‡
        num_kept_tokens = binary_mask.sum(dim=1).float().mean().item()
        
        return {
            'loss': loss,
            'logits': logits,
            'soft_weights': soft_weights,
            'binary_mask': binary_mask,
            'attn_bias': attn_bias,
            'num_kept_tokens': num_kept_tokens
        }


# ==================== è³‡æ–™è™•ç† ====================

def prepare_dataset(dataset, processor, image_dir, label2id, combine_mode='concat'):
    """
    æº–å‚™è³‡æ–™é›†ï¼šè®€å– 2 å¼µèƒŒæ™¯åœ– + 2 å¼µ heatmap
    
    è³‡æ–™æ ¼å¼ï¼š
    {
        'bg_image1': 'èƒŒæ™¯åœ–1',
        'bg_image2': 'èƒŒæ™¯åœ–2',
        'heatmap1': 'heatmap1',
        'heatmap2': 'heatmap2',
        'class': 'é¡åˆ¥',
        'eeg_scalar': 0.5  # å¯é¸
    }
    """
    def preprocess_function(examples):
        batch_size = len(examples['bg_image1'])
        
        # è®€å–èƒŒæ™¯åœ–
        bg_images1 = [Image.open(os.path.join(image_dir, img)).convert('RGB') 
                      for img in examples['bg_image1']]
        bg_images2 = [Image.open(os.path.join(image_dir, img)).convert('RGB') 
                      for img in examples['bg_image2']]
        
        # è®€å– heatmap
        heatmaps1 = [Image.open(os.path.join(image_dir, img)).convert('L') 
                     for img in examples['heatmap1']]
        heatmaps2 = [Image.open(os.path.join(image_dir, img)).convert('L') 
                     for img in examples['heatmap2']]
        
        # è™•ç†èƒŒæ™¯åœ–
        inputs1 = processor(images=bg_images1, return_tensors="pt")
        inputs2 = processor(images=bg_images2, return_tensors="pt")
        
        pixel_values1 = inputs1['pixel_values']
        pixel_values2 = inputs2['pixel_values']
        
        # çµ„åˆèƒŒæ™¯åœ–
        if combine_mode == 'concat':
            combined_pixels = torch.cat([pixel_values1, pixel_values2], dim=1)
        elif combine_mode == 'multiply':
            combined_pixels = pixel_values1 * pixel_values2
        elif combine_mode == 'subtract':
            combined_pixels = pixel_values1 - pixel_values2
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ combine_mode: {combine_mode}")
        
        # è™•ç† heatmap
        heatmap_tensors1 = torch.stack([
            torch.from_numpy(np.array(hm).astype(np.float32) / 255.0) 
            for hm in heatmaps1
        ]).unsqueeze(1)
        
        heatmap_tensors2 = torch.stack([
            torch.from_numpy(np.array(hm).astype(np.float32) / 255.0) 
            for hm in heatmaps2
        ]).unsqueeze(1)
        
        # çµ„åˆ heatmapï¼ˆå¹³å‡ï¼‰
        combined_heatmap = (heatmap_tensors1 + heatmap_tensors2) / 2.0
        
        # Resize heatmap
        target_size = (pixel_values1.shape[-2], pixel_values1.shape[-1])
        combined_heatmap = F.interpolate(
            combined_heatmap, 
            size=target_size, 
            mode='bilinear', 
            align_corners=False
        )
        
        # æ¨™ç±¤
        labels = [label2id[label] for label in examples['class']]
        
        # EEG scalar
        eeg_scalars = examples.get('eeg_scalar', [0.5] * batch_size)
        
        return {
            'pixel_values': combined_pixels,
            'heatmap': combined_heatmap,
            'labels': labels,
            'eeg_scalar': eeg_scalars
        }
    
    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        batch_size=8,
        writer_batch_size=8,  # Add this
        remove_columns=dataset.column_names
    )
    
    processed_dataset.set_format(
        type='torch', 
        columns=['pixel_values', 'heatmap', 'labels', 'eeg_scalar']
    )
    
    return processed_dataset


# ==================== è¨“ç·´å™¨ ====================

class GazeGuidedViTClassifier:
    """Gaze-Guided ViT åˆ†é¡å™¨"""
    
    def __init__(self, num_labels, model_name='google/vit-base-patch16-224', 
                 combine_mode='concat', use_gaze=True, 
                 apply_gaze_from_layer=0, use_pruning=True):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.combine_mode = combine_mode
        self.use_gaze = use_gaze
        
        print(f"\n{'='*60}")
        print(f"åˆå§‹åŒ– Gaze-Guided ViT Classifier")
        print(f"{'='*60}")
        print(f"è£ç½®: {self.device}")
        print(f"åœ–ç‰‡çµ„åˆæ¨¡å¼: {combine_mode}")
        print(f"ä½¿ç”¨ Gaze å¼•å°: {use_gaze}")
        
        # è¼‰å…¥åŸºç¤æ¨¡å‹
        base_model = ViTForImageClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            ignore_mismatched_sizes=True
        )
        
        # ä¿®æ”¹ç¬¬ä¸€å±¤ï¼ˆå¦‚æœæ˜¯ concat æ¨¡å¼ï¼‰
        if combine_mode == 'concat':
            self._modify_first_layer(base_model)
        
        # å»ºç«‹ Gaze-Guided æ¨¡å‹
        if use_gaze:
            self.model = GazeGuidedViT(
                base_model, 
                patch_size=16, 
                img_size=224,
                apply_gaze_from_layer=apply_gaze_from_layer,
                use_pruning=use_pruning
            )
        else:
            self.model = base_model
            print("âœ“ ä½¿ç”¨æ¨™æº– ViTï¼ˆä¸å« Gaze å¼•å°ï¼‰")
        
        self.model.to(self.device)
        self.processor = ViTImageProcessor.from_pretrained(model_name)
        
        print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œåˆ†é¡æ•¸: {num_labels}")
        print(f"{'='*60}\n")
    
    def _modify_first_layer(self, base_model):
        """ä¿®æ”¹ç¬¬ä¸€å±¤ä»¥æ”¯æ´ 6 channels"""
        original_conv = base_model.vit.embeddings.patch_embeddings.projection
        new_conv = nn.Conv2d(
            in_channels=6,
            out_channels=original_conv.out_channels,
            kernel_size=original_conv.kernel_size,
            stride=original_conv.stride,
            padding=original_conv.padding,
            bias=original_conv.bias is not None
        )
        
        with torch.no_grad():
            new_conv.weight[:, :3, :, :] = original_conv.weight.clone()
            new_conv.weight[:, 3:, :, :] = original_conv.weight.clone()
            new_conv.weight.data = new_conv.weight.data / 2.0
            if new_conv.bias is not None and original_conv.bias is not None:
                new_conv.bias.data = original_conv.bias.data.clone()
        
        base_model.vit.embeddings.patch_embeddings.projection = new_conv
        print("âœ“ å·²ä¿®æ”¹ç¬¬ä¸€å±¤æ”¯æ´ 6 channels (concat æ¨¡å¼)")
    
    def train(self, train_dataset, val_dataset=None, epochs=10, 
              batch_size=16, learning_rate=2e-5, 
              top_p=0.7, alpha=1.0,
              save_path='gaze_vit_model.pth'):
        """è¨“ç·´æ¨¡å‹"""
        
        print(f"\n{'='*60}")
        print(f"é–‹å§‹è¨“ç·´")
        print(f"{'='*60}")
        print(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(train_dataset)}")
        if val_dataset:
            print(f"é©—è­‰æ¨£æœ¬æ•¸: {len(val_dataset)}")
        print(f"Epochs: {epochs}")
        print(f"Batch size: {batch_size}")
        print(f"Learning rate: {learning_rate}")
        print(f"Top-p: {top_p}")
        print(f"Alpha: {alpha}")
        print(f"{'='*60}\n")
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=2,
                pin_memory=True
            )
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        best_val_acc = 0.0
        
        for epoch in range(epochs):
            # ===== è¨“ç·´éšæ®µ =====
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            train_kept_tokens = 0
            
            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')
            for batch_idx, batch in enumerate(pbar):
                pixel_values = batch['pixel_values'].to(self.device)
                heatmap = batch['heatmap'].to(self.device)
                labels = batch['labels'].to(self.device)
                eeg_scalar = batch['eeg_scalar'].to(self.device) if torch.is_tensor(batch['eeg_scalar']) else batch['eeg_scalar'][0]
                
                # å‰å‘å‚³æ’­
                if self.use_gaze:
                    outputs = self.model(
                        pixel_values=pixel_values,
                        heatmap=heatmap,
                        eeg_scalar=eeg_scalar,
                        top_p=top_p,
                        alpha=alpha,
                        labels=labels
                    )
                    loss = outputs['loss']
                    logits = outputs['logits']
                    train_kept_tokens += outputs['num_kept_tokens']
                else:
                    outputs = self.model(pixel_values=pixel_values, labels=labels)
                    loss = outputs.loss
                    logits = outputs.logits
                
                # åå‘å‚³æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # çµ±è¨ˆ
                predictions = logits.argmax(dim=-1)
                train_correct += (predictions == labels).sum().item()
                train_total += labels.size(0)
                train_loss += loss.item()
                
                # æ›´æ–°é€²åº¦æ¢
                postfix = {
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100*train_correct/train_total:.2f}%'
                }
                if self.use_gaze:
                    avg_tokens = train_kept_tokens / (batch_idx + 1)
                    postfix['tokens'] = f'{avg_tokens:.1f}'
                
                pbar.set_postfix(postfix)
            
            avg_train_loss = train_loss / len(train_loader)
            train_acc = 100 * train_correct / train_total
            
            print(f'\nEpoch {epoch+1}/{epochs}')
            print(f'  Train Loss: {avg_train_loss:.4f}')
            print(f'  Train Acc:  {train_acc:.2f}%')
            if self.use_gaze:
                avg_tokens = train_kept_tokens / len(train_loader)
                print(f'  Avg Tokens: {avg_tokens:.1f}')
            
            # ===== é©—è­‰éšæ®µ =====
            if val_dataset:
                val_acc = self.evaluate(val_loader, top_p, alpha)
                print(f'  Val Acc:    {val_acc:.2f}%')
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    torch.save(self.model.state_dict(), save_path)
                    print(f'  âœ“ æœ€ä½³æ¨¡å‹å·²å„²å­˜')
            
            print()
        
        if not val_dataset:
            torch.save(self.model.state_dict(), save_path)
            print(f'âœ“ è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å„²å­˜åˆ° {save_path}')
        else:
            print(f'âœ“ è¨“ç·´å®Œæˆï¼æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%')
    
    def evaluate(self, dataloader, top_p=0.7, alpha=1.0):
        """è©•ä¼°æ¨¡å‹"""
        self.model.eval()
        correct = 0
        total = 0
        total_kept_tokens = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc='Evaluating'):
                pixel_values = batch['pixel_values'].to(self.device)
                heatmap = batch['heatmap'].to(self.device)
                labels = batch['labels'].to(self.device)
                eeg_scalar = batch['eeg_scalar'].to(self.device) if torch.is_tensor(batch['eeg_scalar']) else batch['eeg_scalar'][0]
                
                if self.use_gaze:
                    outputs = self.model(
                        pixel_values=pixel_values,
                        heatmap=heatmap,
                        eeg_scalar=eeg_scalar,
                        top_p=top_p,
                        alpha=alpha
                    )
                    predictions = outputs['logits'].argmax(dim=-1)
                    total_kept_tokens += outputs['num_kept_tokens']
                else:
                    outputs = self.model(pixel_values=pixel_values)
                    predictions = outputs.logits.argmax(dim=-1)
                
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        accuracy = 100 * correct / total
        
        if self.use_gaze:
            avg_tokens = total_kept_tokens / len(dataloader)
            print(f'  å¹³å‡ä¿ç•™ tokens: {avg_tokens:.1f}')
        
        return accuracy
    
    def predict(self, bg_image1_path, bg_image2_path, 
                heatmap1_path, heatmap2_path, 
                eeg_scalar=0.5, top_p=0.7, alpha=1.0):
        """
        é æ¸¬å–®ç­†è³‡æ–™
        
        Args:
            bg_image1_path: èƒŒæ™¯åœ– 1 è·¯å¾‘
            bg_image2_path: èƒŒæ™¯åœ– 2 è·¯å¾‘
            heatmap1_path: Heatmap 1 è·¯å¾‘
            heatmap2_path: Heatmap 2 è·¯å¾‘
            eeg_scalar: EEG æ¨™é‡
            top_p: Token ä¿ç•™æ¯”ä¾‹
            alpha: Attention bias å¼·åº¦
        
        Returns:
            predicted_class: é æ¸¬çš„é¡åˆ¥ ID
            confidence: ä¿¡å¿ƒåˆ†æ•¸
            outputs: å®Œæ•´çš„æ¨¡å‹è¼¸å‡ºï¼ˆåŒ…å« gaze è³‡è¨Šï¼‰
        """
        self.model.eval()
        
        # è®€å–èƒŒæ™¯åœ–
        bg1 = Image.open(bg_image1_path).convert('RGB')
        bg2 = Image.open(bg_image2_path).convert('RGB')
        
        # è®€å– heatmap
        hm1 = Image.open(heatmap1_path).convert('L')
        hm2 = Image.open(heatmap2_path).convert('L')
        
        # è™•ç†èƒŒæ™¯åœ–
        inputs1 = self.processor(images=bg1, return_tensors="pt")
        inputs2 = self.processor(images=bg2, return_tensors="pt")
        
        pixel_values1 = inputs1['pixel_values']
        pixel_values2 = inputs2['pixel_values']
        
        # çµ„åˆèƒŒæ™¯åœ–
        if self.combine_mode == 'concat':
            combined_pixels = torch.cat([pixel_values1, pixel_values2], dim=1)
        elif self.combine_mode == 'multiply':
            combined_pixels = pixel_values1 * pixel_values2
        elif self.combine_mode == 'subtract':
            combined_pixels = pixel_values1 - pixel_values2
        
        # è™•ç† heatmap
        hm1_tensor = torch.from_numpy(np.array(hm1).astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
        hm2_tensor = torch.from_numpy(np.array(hm2).astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0)
        combined_heatmap = (hm1_tensor + hm2_tensor) / 2.0
        
        # Resize heatmap
        target_size = (combined_pixels.shape[-2], combined_pixels.shape[-1])
        combined_heatmap = F.interpolate(
            combined_heatmap,
            size=target_size,
            mode='bilinear',
            align_corners=False
        )
        
        # ç§»åˆ° device
        combined_pixels = combined_pixels.to(self.device)
        combined_heatmap = combined_heatmap.to(self.device)
        
        # é æ¸¬
        with torch.no_grad():
            if self.use_gaze:
                outputs = self.model(
                    pixel_values=combined_pixels,
                    heatmap=combined_heatmap,
                    eeg_scalar=eeg_scalar,
                    top_p=top_p,
                    alpha=alpha
                )
                logits = outputs['logits']
            else:
                outputs = self.model(pixel_values=combined_pixels)
                logits = outputs.logits
            
            probabilities = torch.softmax(logits, dim=-1)
            predicted_class = logits.argmax(dim=-1).item()
            confidence = probabilities[0][predicted_class].item()
        
        return predicted_class, confidence, outputs


# ==================== ä¸»ç¨‹å¼ ====================

if __name__ == "__main__":
    # ===== 1. è¨­å®šåƒæ•¸ =====
    IMAGE_DIR = "./"                          # åœ–ç‰‡è³‡æ–™å¤¾
    TRAIN_JSON = "./train_gaze.json"          # è¨“ç·´è³‡æ–™
    VAL_JSON = "./val_gaze.json"              # é©—è­‰è³‡æ–™ï¼ˆå¯é¸ï¼‰
    
    # æ¨¡å‹é…ç½®
    COMBINE_MODE = 'subtract'                 # 'concat', 'multiply', 'subtract'
    USE_GAZE = True                           # æ˜¯å¦ä½¿ç”¨ Gaze å¼•å°
    APPLY_GAZE_FROM_LAYER = 0                 # å¾ç¬¬å¹¾å±¤é–‹å§‹æ‡‰ç”¨ gaze
    USE_PRUNING = True                        # æ˜¯å¦ä½¿ç”¨ token pruning
    
    # è¨“ç·´è¶…åƒæ•¸
    EPOCHS = 10
    BATCH_SIZE = 16
    LEARNING_RATE = 2e-5
    TOP_P = 0.7                               # ä¿ç•™ 70% tokens
    ALPHA = 1.0                               # Attention bias å¼·åº¦
    
    SAVE_PATH = f'gaze_vit_{COMBINE_MODE}_p{TOP_P}.pth'
    
    # ===== 2. è¼‰å…¥è³‡æ–™ =====
    print("è¼‰å…¥è³‡æ–™é›†...")
    train_ds = load_dataset("json", data_files=TRAIN_JSON, split="train")
    
    # å¦‚æœæœ‰é©—è­‰é›†
    val_ds = None
    if os.path.exists(VAL_JSON):
        val_ds = load_dataset("json", data_files=VAL_JSON, split="train")
        print(f"âœ“ è¼‰å…¥é©—è­‰é›†: {len(val_ds)} ç­†")
    
    print(f"âœ“ è¼‰å…¥è¨“ç·´é›†: {len(train_ds)} ç­†")
    
    # ===== 3. å»ºç«‹é¡åˆ¥æ˜ å°„ =====
    unique_classes = set(train_ds['class'])
    label2id = {label: idx for idx, label in enumerate(sorted(unique_classes))}
    id2label = {idx: label for label, idx in label2id.items()}
    
    print(f"\né¡åˆ¥æ˜ å°„:")
    for label, idx in sorted(label2id.items(), key=lambda x: x[1]):
        print(f"  {idx}: {label}")
    print(f"ç¸½é¡åˆ¥æ•¸: {len(label2id)}\n")
    
    # ===== 4. åˆå§‹åŒ–æ¨¡å‹ =====
    classifier = GazeGuidedViTClassifier(
        num_labels=len(label2id),
        combine_mode=COMBINE_MODE,
        use_gaze=USE_GAZE,
        apply_gaze_from_layer=APPLY_GAZE_FROM_LAYER,
        use_pruning=USE_PRUNING
    )
    
    # ===== 5. é è™•ç†è³‡æ–™é›† =====
    print("é è™•ç†è¨“ç·´è³‡æ–™...")
    train_dataset = prepare_dataset(
        train_ds, 
        classifier.processor, 
        IMAGE_DIR, 
        label2id,
        combine_mode=COMBINE_MODE
    )
    print(f"âœ“ è¨“ç·´é›†é è™•ç†å®Œæˆ: {len(train_dataset)} ç­†")
    
    val_dataset = None
    if val_ds is not None:
        print("é è™•ç†é©—è­‰è³‡æ–™...")
        val_dataset = prepare_dataset(
            val_ds, 
            classifier.processor, 
            IMAGE_DIR, 
            label2id,
            combine_mode=COMBINE_MODE
        )
        print(f"âœ“ é©—è­‰é›†é è™•ç†å®Œæˆ: {len(val_dataset)} ç­†")
    
    # ===== 6. è¨“ç·´æ¨¡å‹ =====
    classifier.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        top_p=TOP_P,
        alpha=ALPHA,
        save_path=SAVE_PATH
    )
    
    # ===== 7. å„²å­˜é…ç½® =====
    config = {
        'label2id': label2id,
        'id2label': id2label,
        'combine_mode': COMBINE_MODE,
        'use_gaze': USE_GAZE,
        'apply_gaze_from_layer': APPLY_GAZE_FROM_LAYER,
        'use_pruning': USE_PRUNING,
        'top_p': TOP_P,
        'alpha': ALPHA,
        'model_name': 'google/vit-base-patch16-224',
        'num_labels': len(label2id)
    }
    
    config_path = f'gaze_model_config_{COMBINE_MODE}.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ æ¨¡å‹é…ç½®å·²å„²å­˜åˆ° {config_path}")
    
    # ===== 8. é æ¸¬ç¯„ä¾‹ =====
    if len(train_ds) > 0:
        print("\n" + "="*60)
        print("é æ¸¬ç¯„ä¾‹")
        print("="*60)
        
        sample = train_ds[0]
        bg1_path = os.path.join(IMAGE_DIR, sample['bg_image1'])
        bg2_path = os.path.join(IMAGE_DIR, sample['bg_image2'])
        hm1_path = os.path.join(IMAGE_DIR, sample['heatmap1'])
        hm2_path = os.path.join(IMAGE_DIR, sample['heatmap2'])
        
        if all(os.path.exists(p) for p in [bg1_path, bg2_path, hm1_path, hm2_path]):
            predicted_class, confidence, outputs = classifier.predict(
                bg1_path, bg2_path, hm1_path, hm2_path,
                eeg_scalar=sample.get('eeg_scalar', 0.5),
                top_p=TOP_P,
                alpha=ALPHA
            )
            
            predicted_label = id2label[predicted_class]
            actual_label = sample['class']
            
            print(f"èƒŒæ™¯åœ– 1:     {sample['bg_image1']}")
            print(f"èƒŒæ™¯åœ– 2:     {sample['bg_image2']}")
            print(f"Heatmap 1:    {sample['heatmap1']}")
            print(f"Heatmap 2:    {sample['heatmap2']}")
            print(f"å¯¦éš›é¡åˆ¥:     {actual_label}")
            print(f"é æ¸¬é¡åˆ¥:     {predicted_label}")
            print(f"ä¿¡å¿ƒåˆ†æ•¸:     {confidence:.4f}")
            
            if USE_GAZE:
                print(f"ä¿ç•™ tokens:  {outputs['num_kept_tokens']:.1f}")
                print(f"Bias ç¯„åœ:    [{outputs['attn_bias'].min():.2f}, {outputs['attn_bias'].max():.2f}]")
        else:
            print("æ‰¾ä¸åˆ°ç¯„ä¾‹åœ–ç‰‡æª”æ¡ˆ")
    
    print("\n" + "="*60)
    print("æ‰€æœ‰æµç¨‹å®Œæˆï¼")
    print("="*60)
    
    # ===== 9. å¯¦é©—å»ºè­° =====
    print("\nğŸ’¡ å¯¦é©—å»ºè­°ï¼š")
    print("1. å°æ¯”å¯¦é©—ï¼š")
    print("   - Baseline: USE_GAZE=False")
    print("   - Gaze (no pruning): USE_GAZE=True, USE_PRUNING=False")
    print("   - Gaze + Pruning: USE_GAZE=True, USE_PRUNING=True")
    print()
    print("2. åƒæ•¸èª¿æ•´ï¼š")
    print("   - TOP_P: [0.3, 0.5, 0.7, 0.9] æ¸¬è©¦ä¸åŒå‰ªæç¨‹åº¦")
    print("   - ALPHA: [0.5, 1.0, 2.0, 5.0] æ¸¬è©¦ bias å¼·åº¦")
    print("   - APPLY_GAZE_FROM_LAYER: [0, 6] æ¸¬è©¦æ‡‰ç”¨å±¤æ•¸")
    print()
    print("3. è¦–è¦ºåŒ–ï¼š")
    print("   - å¯è¦–åŒ– binary_mask æŸ¥çœ‹ä¿ç•™çš„ patches")
    print("   - ç¹ªè£½ä¸åŒ EEG å€¼ä¸‹çš„æ¬Šé‡åˆ†ä½ˆ")
    print("   - åˆ†æ attention bias çš„å½±éŸ¿")